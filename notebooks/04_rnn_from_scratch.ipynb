{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25154d3b",
   "metadata": {},
   "source": [
    "# BareBonesML Part 4: Recurrent Neural Networks from Scratch\n",
    "\n",
    "The feed-forward networks we've built so far are powerful, but they are stateless. They process each input independently, with no memory of what came before. They cannot understand the order in a sentence or the trend in a time series.\n",
    "\n",
    "Enter **Recurrent Neural Networks (RNNs)**.\n",
    "\n",
    "An RNN is a type of neural network designed to handle sequential data. It does this by introducing a **loop**. At each step in a sequence, the network processes an input and combines it with information from the *previous* step. This \"memory\" of the past is stored in a vector called the **hidden state**.\n",
    "\n",
    "![RNN Unrolled Diagram](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "Credit: [Colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "The core formula that defines a simple RNN is beautifully elegant:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(x_t W_{xh} + h_{t-1} W_{hh} + b_h)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-   $h_t$ is the **new hidden state** (the output for the current step).\n",
    "-   $x_t$ is the **input vector** at the current time step.\n",
    "-   $h_{t-1}$ is the **hidden state from the previous step**.\n",
    "-   $W_{xh}$, $W_{hh}$, and $b_h$ are the learnable weight and bias parameters that are **shared across all time steps**. This weight sharing is the key that allows the RNN to generalize across sequences of different lengths.\n",
    "\n",
    "In this post, we will:\n",
    "1.  Implement a simple `RecurrentBlock` and the `tanh` activation function from scratch.\n",
    "2.  Build a character-level RNN model.\n",
    "3.  Train it to generate new, unique names based on a list of thousands of examples.\n",
    "\n",
    "All code for this post can be found in the `from_scratch/` directory of the [bare-bones-ml repository](https://github.com/devansh-lodha/bare-bones-ml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2124e77",
   "metadata": {},
   "source": [
    "## New Library Components\n",
    "\n",
    "To build our RNN, we only need to add two new pieces to our library.\n",
    "\n",
    "### 1. The `tanh` Activation Function\n",
    "\n",
    "The hyperbolic tangent (`tanh`) function is the traditional non-linearity used in simple RNNs. It squashes its input to a range between -1 and 1. Its derivative is simple: $\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)$, which is important for backpropagation.\n",
    "\n",
    "```python\n",
    "# from_scratch/functional.py\n",
    "\n",
    "class Tanh(Function):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        output = np.tanh(x)\n",
    "        self.save_for_backward(output)\n",
    "        return output\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        output, = self.saved_tensors\n",
    "        return grad * (1 - output**2)\n",
    "\n",
    "def tanh(x: Tensor) -> Tensor:\n",
    "    return Tanh.apply(x)\n",
    "```\n",
    "\n",
    "### 2. The `RecurrentBlock` Module\n",
    "\n",
    "Our `RecurrentBlock` will implement a **single step** of the RNN forward pass. The looping over the sequence will happen explicitly in our training code. This allows us to realize the process of passing the hidden state from one time step to the next, which is often hidden inside a framework's implementation.\n",
    "\n",
    "We also make a design choice to combine the weights `W_xh` and `W_hh` into a single, larger weight matrix `w`. We then concatenate the input `x` and the previous hidden state `h_prev` to perform a single, efficient matrix multiplication.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "\n",
    "class RecurrentBlock(Module):\n",
    "    \"\"\"A simple Recurrent Neural Network (RNN) block.\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        concat_size = input_size + hidden_size\n",
    "        self.w = Tensor(np.random.randn(concat_size, hidden_size) * 0.01, requires_grad=True)\n",
    "        self.b_h = Tensor(np.zeros(hidden_size), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: Tensor, h_prev: Tensor) -> Tensor:\n",
    "        \"\"\"Performs one step of the RNN computation.\"\"\"\n",
    "        combined = Tensor.cat([x, h_prev], axis=1)\n",
    "        h_next = tanh(combined @ self.w + self.b_h)\n",
    "        return h_next\n",
    "\n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        return [self.w, self.b_h]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e65526",
   "metadata": {},
   "source": [
    "## The Task: Character-Level Name Generation\n",
    "\n",
    "We will train our RNN on a list of thousands of names. The model will learn by reading a name one character at a time and trying to predict the *next* character in the sequence. For the name \"emma\", the training examples would be:\n",
    "-   Given a start token `.`, predict \"e\".\n",
    "-   Given \"e\", predict \"m\".\n",
    "-   Given \"m\", predict \"m\".\n",
    "-   Given \"m\", predict \"a\".\n",
    "-   Given \"a\", predict the end token `.`.\n",
    "\n",
    "By training on this task, the model learns the statistical patterns of names: which letters tend to follow others, common prefixes and suffixes, and typical name lengths.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "First, we need to load the data and create our character-level vocabulary. For this task, the \"vocabulary\" is simply the set of all unique characters present in the dataset, plus a special token (`.`) to signify the start and end of a name. We will represent each character as a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1c326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32033 names. First 5: ['emma', 'olivia', 'ava', 'isabella', 'sophia']\n",
      "\n",
      "Vocabulary size: 27\n",
      "Characters: .abcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Example Input Shape: (6, 27)\n",
      "Example Target Shape: (6,)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from from_scratch.autograd.tensor import Tensor\n",
    "\n",
    "# 1. The Dataset\n",
    "# Download from: https://github.com/karpathy/makemore/blob/master/names.txt\n",
    "# And place it in a `data/` directory at the root of your project.\n",
    "\n",
    "with open('/data/names.txt', 'r') as f:\n",
    "    names = [name.lower() for name in f.read().splitlines()]\n",
    "\n",
    "\n",
    "if names:\n",
    "    print(f\"Loaded {len(names)} names. First 5: {names[:5]}\")\n",
    "\n",
    "    # 2. Create the Vocabulary\n",
    "    chars = sorted(list(set(\".\" + \"\".join(names))))\n",
    "    char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "    int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "    print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "    print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "    # 3. Helper to create a training example (input sequence and target sequence)\n",
    "    def get_random_example():\n",
    "        name = random.choice(names)\n",
    "        full_sequence = \".\" + name + \".\"\n",
    "        \n",
    "        input_indices = [char_to_int[c] for c in full_sequence[:-1]]\n",
    "        target_indices = [char_to_int[c] for c in full_sequence[1:]]\n",
    "        \n",
    "        # One-hot encode the input sequence\n",
    "        input_one_hot = np.zeros((len(input_indices), vocab_size), dtype=np.float32)\n",
    "        input_one_hot[np.arange(len(input_indices)), input_indices] = 1\n",
    "\n",
    "        return Tensor(input_one_hot), Tensor(np.array(target_indices))\n",
    "\n",
    "    # Test the helper\n",
    "    test_input, test_target = get_random_example()\n",
    "    print(f\"\\nExample Input Shape: {test_input.shape}\")\n",
    "    print(f\"Example Target Shape: {test_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136052d",
   "metadata": {},
   "source": [
    "## The Model and Training Loop\n",
    "\n",
    "Our full model will consist of our `RecurrentBlock` and a `Linear` output layer. At each time step, the RNN processes a character and updates its hidden state. This hidden state is then passed to the `Linear` layer to produce **logits**â€”raw scores for each character in our vocabulary. We use `cross_entropy` loss to compare these logits to the true next character.\n",
    "\n",
    "The key to training an RNN is that the total loss for a sequence is the **sum of the losses at each time step**. We then call `backward()` on this final accumulated loss to compute gradients for the entire unrolled sequence. This process is called **Backpropagation Through Time (BPTT)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f798f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Start ---\n",
      "Epoch 0, Avg Loss: 3.2939\n",
      "Epoch 1000, Avg Loss: 2.2758\n",
      "Epoch 2000, Avg Loss: 2.2232\n",
      "Epoch 3000, Avg Loss: 2.0630\n",
      "Epoch 4000, Avg Loss: 2.4795\n",
      "Epoch 5000, Avg Loss: 2.5284\n",
      "Epoch 6000, Avg Loss: 2.7203\n",
      "Epoch 7000, Avg Loss: 2.6240\n",
      "Epoch 8000, Avg Loss: 2.6727\n",
      "Epoch 9000, Avg Loss: 3.1646\n",
      "Epoch 10000, Avg Loss: 3.0949\n",
      "Epoch 11000, Avg Loss: 1.9882\n",
      "Epoch 12000, Avg Loss: 3.0557\n",
      "Epoch 13000, Avg Loss: 2.3218\n",
      "Epoch 14000, Avg Loss: 3.1589\n",
      "Epoch 15000, Avg Loss: 2.4487\n",
      "Epoch 16000, Avg Loss: 1.7396\n",
      "Epoch 17000, Avg Loss: 2.7295\n",
      "Epoch 18000, Avg Loss: 2.9926\n",
      "Epoch 19000, Avg Loss: 3.1630\n",
      "Epoch 19999, Avg Loss: 1.9218\n"
     ]
    }
   ],
   "source": [
    "from from_scratch.nn import RecurrentBlock, Linear\n",
    "from from_scratch.optim import Adam\n",
    "from from_scratch.functional import cross_entropy\n",
    "\n",
    "if names:\n",
    "    # Model Definition\n",
    "    hidden_size = 128\n",
    "    rnn_layer = RecurrentBlock(input_size=vocab_size, hidden_size=hidden_size)\n",
    "    output_layer = Linear(input_size=hidden_size, output_size=vocab_size)\n",
    "\n",
    "    # Group all parameters for the optimizer\n",
    "    all_params = rnn_layer.parameters() + output_layer.parameters()\n",
    "    optimizer = Adam(params=all_params, lr=0.005)\n",
    "\n",
    "    # Training Loop\n",
    "    epochs = 20000\n",
    "    print_every = 1000\n",
    "    \n",
    "    print(\"\\n--- Training Start ---\")\n",
    "    for epoch in range(epochs):\n",
    "        input_tensor, target_tensor = get_random_example()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hidden = Tensor(np.zeros((1, hidden_size)))\n",
    "        total_loss = Tensor(0.0)\n",
    "        \n",
    "        # Explicitly loop through the sequence (BPTT)\n",
    "        for t in range(input_tensor.shape[0]):\n",
    "            x_t = input_tensor[t:t+1, :]\n",
    "            \n",
    "            # RNN step: Pass input and previous hidden state\n",
    "            hidden = rnn_layer(x_t, hidden)\n",
    "            \n",
    "            # Output layer to get prediction for this step\n",
    "            logits = output_layer(hidden)\n",
    "            \n",
    "            # Calculate loss for this step\n",
    "            target_t = target_tensor[t:t+1]\n",
    "            loss = cross_entropy(logits, target_t)\n",
    "            \n",
    "            # Accumulate the loss\n",
    "            total_loss = total_loss + loss\n",
    "\n",
    "        # Backward pass on the final accumulated loss for the whole sequence\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        for p in all_params:\n",
    "            if p.grad is not None:\n",
    "                np.clip(p.grad, -5, 5, out=p.grad)\n",
    "                \n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % print_every == 0 or epoch == epochs - 1:\n",
    "            avg_loss = total_loss.data.item() / input_tensor.shape[0]\n",
    "            print(f\"Epoch {epoch}, Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91a03f",
   "metadata": {},
   "source": [
    "## Generating New Names\n",
    "\n",
    "The reward for training a generative model is seeing what it creates! To generate a name, we give the model a starting letter and an initial hidden state of zeros. We then enter a loop:\n",
    "1.  Feed the current character and hidden state to the model to get logits for the next character.\n",
    "2.  Convert the logits to probabilities using `softmax`.\n",
    "3.  Sample a character from this probability distribution.\n",
    "4.  Feed this new character as the input for the next time step.\n",
    "5.  Repeat until the model generates the special \"end\" token (`.`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c17df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Names ---\n",
      "an\n",
      "ban\n",
      "caylsa\n",
      "deo\n",
      "el\n"
     ]
    }
   ],
   "source": [
    "from from_scratch.functional import softmax\n",
    "\n",
    "def generate_name(start_letter='a', max_len=20):\n",
    "    def char_to_tensor(char):\n",
    "        tensor = np.zeros((1, vocab_size))\n",
    "        tensor[0, char_to_int[char]] = 1\n",
    "        return Tensor(tensor)\n",
    "\n",
    "    hidden = Tensor(np.zeros((1, hidden_size)))\n",
    "    current_input = char_to_tensor(start_letter)\n",
    "    name = start_letter\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        hidden = rnn_layer(current_input, hidden)\n",
    "        logits = output_layer(hidden)\n",
    "        \n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        next_char_idx = np.random.choice(len(chars), p=probs.data.flatten())\n",
    "        \n",
    "        if int_to_char[next_char_idx] == '.':\n",
    "            break\n",
    "            \n",
    "        next_char = int_to_char[next_char_idx]\n",
    "        name += next_char\n",
    "        current_input = char_to_tensor(next_char)\n",
    "            \n",
    "    return name\n",
    "\n",
    "print(\"\\n--- Generated Names ---\")\n",
    "for char in \"abcde\":\n",
    "    print(generate_name(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f98e0",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "We have successfully built our first stateful model! It can process inputs of varying lengths and generate new sequences that capture the statistical patterns of the training data.\n",
    "\n",
    "However, simple RNNs suffer from a major issue: the **vanishing gradient problem**. As sequences get longer, it becomes very difficult for gradients to flow back to the beginning of the sequence, making it hard for the model to learn long-range dependencies (e.g., how the first letter of a name influences the last).\n",
    "\n",
    "In our next post, we will tackle this problem by implementing a more advanced and powerful recurrent architecture: the **Long Short-Term Memory (LSTM)** network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bare-bones-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
