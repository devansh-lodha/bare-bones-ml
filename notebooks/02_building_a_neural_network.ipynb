{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92242ce4",
   "metadata": {},
   "source": [
    "# BareBonesML Part 2: From Tensors to a Trainable Neural Network\n",
    "\n",
    "In our [previous post](notebooks/01_autograd_engine.ipynb), we built an autograd engine. We have `Tensor` objects that can track their computational history and automatically compute gradients via backpropagation.\n",
    "\n",
    "Today, we will use that engine to build an initial deep learning library. We'll move up one level of abstraction to create reusable **layers** and an **optimizer**.\n",
    "\n",
    "Our goals for this post are:\n",
    "1. Create a `Module` class inspired by `torch.nn.Module`.\n",
    "2. Implement a `Linear` layer\n",
    "3. Build a `Sequential` container to easily stack our layers into a deep model.\n",
    "4. Implement Stochastic Gradient Descent (SGD) optimizer.\n",
    "5. Train a simple model on a toy regression problem.\n",
    "\n",
    "All the code for this post can be found in the `from_scratch/` directory of the [bare-bones-ml repository](https://github.com/devansh-lodha/bare-bones-ml).\n",
    "\n",
    "## The `Module`: Our Neural Network Building Block\n",
    "\n",
    "A `Module` is the core container in our neural network library. Its job is to keep track of all the learnable parameters (`Tensor`s with `requires_grad=True`) within it. A `Module` can also contain other `Module`s, allowing us to build complex, nested model architectures.\n",
    "\n",
    "The magic happens in the `__setattr__` method. We override this special Python method so that whenever we assign a `Tensor` or another `Module` as an attribute, it gets automatically registered.\n",
    "\n",
    "Here is the implementation from `from_scratch/nn.py`:\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        # Every module starts with two empty dictionaries\n",
    "        self._parameters = {} # For Tensors\n",
    "        self._modules = {}    # For other Modules\n",
    "\n",
    "    def __setattr__(self, name: str, value):\n",
    "        # This method is called EVERY time we do `self.some_name = some_value`\n",
    "        \n",
    "        # We check the TYPE of the value we are assigning.\n",
    "        if isinstance(value, Tensor):\n",
    "            # If it's a Tensor, we add it to our internal `_parameters` dictionary.\n",
    "            print(f\"Registering parameter '{name}'\")\n",
    "            self._parameters[name] = value\n",
    "\n",
    "        elif isinstance(value, Module):\n",
    "            # If it's another Module (like a Linear layer), we add it to `_modules`.\n",
    "            print(f\"Registering module '{name}'\")\n",
    "            self._modules[name] = value\n",
    "        \n",
    "        # We still call the original, default Python __setattr__ to actually perform the assignment (i.e., to make `self.layer1` exist).\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a list of all parameters in this module AND all its sub-modules.\n",
    "        \"\"\"\n",
    "        # 1. Start with the parameters defined directly in this module.\n",
    "        params = list(self._parameters.values())\n",
    "        \n",
    "        # 2. Go into each registered sub-module...\n",
    "        for module in self._modules.values():\n",
    "            # ...and ask it for its parameters (which will also be a recursive call).\n",
    "            params.extend(module.parameters())\n",
    "            \n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Resets the gradients of all parameters to None.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.grad = None\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Allows the module to be called like a function, which runs the forward pass.\"\"\"\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Forward pass. Must be implemented by subclasses.\"\"\"\n",
    "        raise NotImplementedError\n",
    "```\n",
    "### `__setattr__` and Automatic Parameter Registration\n",
    "\n",
    "You might wonder how `model.parameters()` magically finds all the `Tensor`s from all the layers. This is not magic, but a clever use of a special Python method called `__setattr__`.\n",
    "\n",
    "Whenever you assign an attribute in Python, like `self.weight = ...`, Python internally calls `self.__setattr__('weight', ...)`. We have *overridden* this method in our base `Module` class to act as a registrar.\n",
    "\n",
    "Our custom `__setattr__` checks the type of the value being assigned. If it's a `Tensor`, it gets added to an internal `_parameters` dictionary. If it's another `Module`, it gets added to `_modules`.\n",
    "\n",
    "The `parameters()` method then simply traverses this structure, collecting all the parameters from a module and recursively asking all of its sub-modules for their parameters. This elegant design allows us to define complex, nested architectures while the framework handles the tedious bookkeeping of all the learnable weights and biases for us.\n",
    "\n",
    "## The `Linear` Layer\n",
    "\n",
    "With `Module` as our base, creating a `Linear` layer is incredibly elegant. A linear layer performs the affine transformation $y = xW^T + b$. All it needs to do is:\n",
    "1.  Inherit from `Module`.\n",
    "2.  In its `__init__`, create its learnable `weight` and `bias` Tensors. Because of our clever `__setattr__`, they are automatically registered as parameters.\n",
    "3.  Define the `forward` method to perform the matrix multiplication and addition.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"A standard fully-connected linear layer: y = xW^T + b\"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        super().__init__() # This initializes the parent Module\n",
    "        self.weight = Tensor(\n",
    "            np.random.randn(output_size, input_size) * np.sqrt(2.0 / input_size), \n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.bias = Tensor(np.zeros(output_size), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x @ self.weight.T + self.bias\n",
    "```\n",
    "### Design Deep Dive: Why He Initialization?\n",
    "\n",
    "In the `Linear` layer, you'll notice we don't just use `np.random.randn()`. We scale it:\n",
    "```python\n",
    "self.weight = Tensor(\n",
    "    np.random.randn(output_size, input_size) * np.sqrt(2.0 / input_size), \n",
    "    ...\n",
    ")\n",
    "```\n",
    "This is a crucial technique called **He Initialization**, named after its inventor, Kaiming He.\n",
    "\n",
    "**The Problem: Vanishing and Exploding Gradients**\n",
    "\n",
    "When we stack many layers, the variance of the outputs can change at each layer. If the weights are too small, the signal (and gradients) can shrink to zero as it passes through the network, which is known as the **vanishing gradient problem**. If the weights are too large, the signal can grow exponentially, leading to the **exploding gradient problem**.\n",
    "\n",
    "**The Solution: Scaling by Fan-in**\n",
    "\n",
    "He initialization sets the initial weights to just the right scale by accounting for the **fan-in** of the layer. The \"fan-in\" is simply the number of input connections to a neuron in that layer. For a `Linear` layer, the fan-in is its `input_size`.\n",
    "\n",
    "The logic is that the output of a neuron is a sum of `fan_in` terms. The more terms you add together, the larger the variance of the sum will be. To counteract this and keep the variance stable across layers, we need to make the individual weights smaller for layers with more inputs.\n",
    "\n",
    "The He initialization formula does this perfectly:\n",
    "$$\n",
    "\\text{scale} = \\sqrt{\\frac{2}{\\text{fan\\_in}}}\n",
    "$$\n",
    "\n",
    "By scaling our random weights by this factor, we help keep the signal flowing smoothly forward and backward, allowing us to train much deeper networks successfully. The `2` in the numerator is specifically derived to work best with the ReLU activation function, which is why this combination is so common in modern networks.\n",
    "\n",
    "## The `Sequential` Container\n",
    "\n",
    "Models are often just a sequence of layers. A `Sequential` module is a container that takes other modules and applies them in order during the forward pass. This makes defining a standard feed-forward network clean and easy.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "\n",
    "class Sequential(Module):\n",
    "    \"\"\"A container for modules that will be applied in sequence.\"\"\"\n",
    "    def __init__(self, *modules: Module):\n",
    "        super().__init__()\n",
    "        for i, module in enumerate(modules):\n",
    "            # The name of the submodule will be its index as a string\n",
    "            self._modules[str(i)] = module\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Passes the input through each module in order.\"\"\"\n",
    "        for module in self._modules.values():\n",
    "            x = module(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "Now let's see how easy it is to define a 2-hidden-layer MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd5e4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Sequential model found 6 parameter Tensors.\n",
      "Total number of learnable parameters: 41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from from_scratch.autograd.tensor import Tensor\n",
    "from from_scratch.nn import Linear, Sequential, ReLU\n",
    "\n",
    "# Define a 2-hidden-layer MLP using our library components\n",
    "# Input (3 features) -> Hidden1 (4 neurons) -> ReLU -> Hidden2 (4 neurons) -> ReLU -> Output (1 neuron)\n",
    "model = Sequential(\n",
    "    Linear(input_size=3, output_size=4),\n",
    "    ReLU(),\n",
    "    Linear(input_size=4, output_size=4),\n",
    "    ReLU(),\n",
    "    Linear(input_size=4, output_size=1)\n",
    ")\n",
    "\n",
    "# Thanks to our Module's recursive parameter collection, this just works!\n",
    "all_params = model.parameters()\n",
    "print(f\"Our Sequential model found {len(all_params)} parameter Tensors.\")\n",
    "\n",
    "# Let's inspect the number of parameters per layer\n",
    "# Layer 1: weight (3x4) + bias (4) = 12 + 4 = 16\n",
    "# Layer 2: weight (4x4) + bias (4) = 16 + 4 = 20\n",
    "# Layer 3: weight (4x1) + bias (1) = 4 + 1 = 5\n",
    "# Total params = 16 + 20 + 5 = 41\n",
    "num_params_total = sum(p.data.size for p in all_params)\n",
    "print(f\"Total number of learnable parameters: {num_params_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143eae2",
   "metadata": {},
   "source": [
    "## The Optimizer\n",
    "\n",
    "The final piece of the puzzle is the **optimizer**. Its job is simple: it holds a reference to the model's parameters and updates them using their computed gradients.\n",
    "\n",
    "We'll start with the most fundamental optimizer: **Stochastic Gradient Descent (SGD)**. Its update rule is the one we saw earlier: `parameter -= learning_rate * gradient`.\n",
    "\n",
    "```python\n",
    "# from_scratch/optim.py\n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"Base class for all optimizers.\"\"\"\n",
    "    def __init__(self, params: List[Tensor], lr: float):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Resets the gradients of all parameters.\"\"\"\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Implements Stochastic Gradient Descent optimizer.\"\"\"\n",
    "    def __init__(self, params: List[Tensor], lr: float):\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.data -= self.lr * p.grad\n",
    "```\n",
    "\n",
    "## The Full Training Loop\n",
    "\n",
    "We now have all the components to train a neural network. Let's put them together to solve a simple regression task. We'll train our model to learn the function $y = 2x_1 + 3x_2$.\n",
    "\n",
    "The training loop follows a standard recipe:\n",
    "1.  Get model predictions (forward pass).\n",
    "2.  Compute the loss (how wrong the predictions are).\n",
    "3.  Compute the gradients (backward pass).\n",
    "4.  Update the model parameters using the optimizer.\n",
    "5.  Repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b47d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Start ---\n",
      "Epoch 0, Loss: 328.5437\n",
      "Epoch 5, Loss: 7.4500\n",
      "Epoch 10, Loss: 0.3352\n",
      "Epoch 15, Loss: 0.1724\n",
      "Epoch 19, Loss: 0.1648\n",
      "\n",
      "--- Final Predictions vs True Values ---\n",
      "  True: 5.00, Predicted: 5.65\n",
      "  True: 10.00, Predicted: 10.32\n",
      "  True: 15.00, Predicted: 14.98\n",
      "  True: 20.00, Predicted: 19.64\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports and Data\n",
    "import numpy as np\n",
    "from from_scratch.autograd.tensor import Tensor\n",
    "from from_scratch.nn import Linear\n",
    "from from_scratch.functional import mse_loss\n",
    "from from_scratch.optim import SGD\n",
    "\n",
    "# Toy dataset\n",
    "# X shape: (num_samples, num_features)\n",
    "X = Tensor(np.array([[1, 1], [2, 2], [3, 3], [4, 4]], dtype=np.float32))\n",
    "# y = 2*x1 + 3*x2. For [1,1], y=5. For [2,2], y=10, etc.\n",
    "y_true = Tensor(np.array([[5], [10], [15], [20]], dtype=np.float32))\n",
    "\n",
    "# 2. Model, Loss, and Optimizer\n",
    "model = Linear(input_size=2, output_size=1)\n",
    "loss_function = mse_loss\n",
    "optimizer = SGD(params=model.parameters(), lr=0.01)\n",
    "\n",
    "# 3. The Training Loop\n",
    "epochs = 20\n",
    "print(\"--- Training Start ---\")\n",
    "for epoch in range(epochs):\n",
    "    # a. Zero out gradients from the last step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # b. Forward pass: get model predictions\n",
    "    predictions = model(X)\n",
    "\n",
    "    # c. Compute the loss\n",
    "    loss = loss_function(predictions, y_true)\n",
    "\n",
    "    # d. Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # e. Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.data.item():.4f}\")\n",
    "\n",
    "# 4. Check the final predictions\n",
    "print(\"\\n--- Final Predictions vs True Values ---\")\n",
    "final_predictions = model(X)\n",
    "for true, pred in zip(y_true.data, final_predictions.data):\n",
    "    print(f\"  True: {true[0]:.2f}, Predicted: {pred[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccfad4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The loss steadily decreases, and the model's final predictions are very close to the true values.\n",
    "\n",
    "By creating a `Module` class, we were able to abstract away the low-level details of our autograd engine and build reusable `Linear` and `Sequential` layers. This is the essence of a deep learning library. We now have a clean, intuitive way to define model architectures.\n",
    "\n",
    "In the next post, we'll expand our library to tackle a real-world classification problem, which will require us to implement a more advanced optimizer (`Adam`) and new loss and activation functions (`BinaryCrossEntropy` and `Sigmoid`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bare-bones-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
