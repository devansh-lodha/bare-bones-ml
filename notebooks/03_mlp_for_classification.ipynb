{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c82ac1",
   "metadata": {},
   "source": [
    "# BareBonesML Part 3: Building a Multi-Layer Perceptron for Classification\n",
    "\n",
    "In the [last post](notebooks/02_building_a_neural_network.ipynb), we built the foundational `Module`, `Linear` layer, and `Sequential` container. We successfully trained a single-layer model on a toy regression problem. Now, it's time to unlock the true power of neural networks by stacking multiple layers and tackling a real-world **binary classification** task.\n",
    "\n",
    "Our goals for this post are:\n",
    "1. Implement `Sigmoid` activation function and `Binary Cross-Entropy (BCE)` loss to our library.\n",
    "2. Implement `Adam` optimizer.\n",
    "3. Use our `Sequential` container to easily stack `Linear` layers and activations to build a multi-layer perceptron (MLP).\n",
    "4. Use the Wisconsin Breast Cancer dataset from Scikit-learn to train our from-scratch MLP.\n",
    "\n",
    "All the code for this post can be found in the `from_scratch/` directory of the [bare-bones-ml repository](https://github.com/devansh-lodha/bare-bones-ml).\n",
    "\n",
    "## New Tools for a New Task: Classification\n",
    "\n",
    "Regression (predicting a continuous value) is different from classification (predicting a discrete category). This requires a different set of tools. For a binary (yes/no) problem, we need two things: a way to output a probability, and a way to measure the error of that probability.\n",
    "\n",
    "### 1. The `Sigmoid` Activation Function\n",
    "\n",
    "To get a probability, we need to squash the model's raw output (called a \"logit,\" which can be any real number) into the range `[0, 1]`. The `Sigmoid` function is perfect for this.\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "Its derivative is also simple and elegant, which is useful for backpropagation:\n",
    "\n",
    "$$ \\frac{d\\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x)) $$\n",
    "\n",
    "Here is the implementation in `from_scratch/functional.py`:\n",
    "\n",
    "```python\n",
    "# from_scratch/functional.py\n",
    "class Sigmoid(Function):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        output = 1 / (1 + np.exp(-x))\n",
    "        self.save_for_backward(output)\n",
    "        return output\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        output, = self.saved_tensors\n",
    "        return grad * output * (1 - output)\n",
    "\n",
    "def sigmoid(x: Tensor) -> Tensor:\n",
    "    return Sigmoid.apply(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5061de8",
   "metadata": {},
   "source": [
    "### 2. Binary Cross-Entropy (BCE) Loss\n",
    "\n",
    "`MSELoss` is a poor choice for measuring the error of a probability. If the model predicts `0.9` and the true label is `1`, the squared error is small. But if it predicts `0.1` (very confident and very wrong), we want to penalize it heavily.\n",
    "\n",
    "**Binary Cross-Entropy (BCE)** does exactly this. It heavily penalizes confident, incorrect predictions. For a single prediction, the formula is:\n",
    "\n",
    "$$\n",
    "L = - \\left( y_{\\text{true}} \\log(\\hat{y}_{\\text{pred}}) + (1 - y_{\\text{true}}) \\log(1 - \\hat{y}_{\\text{pred}}) \\right)\n",
    "$$\n",
    "\n",
    "Where $\\hat{y}_{\\text{pred}}$ is the model's predicted probability (the output of the sigmoid).\n",
    "\n",
    "```python\n",
    "# from_scratch/functional.py\n",
    "class BCELoss(Function):\n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        epsilon = 1e-15 # Clip predictions to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        self.save_for_backward(y_pred, y_true)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return np.array(loss)\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        y_pred, y_true = self.saved_tensors\n",
    "        n = y_pred.shape if y_pred.ndim > 0 else 1\n",
    "        # Derivative of BCE Loss\n",
    "        grad_y_pred = grad * (1.0 / n) * ((y_pred - y_true) / (y_pred * (1 - y_pred)))\n",
    "        return grad_y_pred, None # No gradient for the true labels\n",
    "```\n",
    "\n",
    "### 3. The Adam Optimizer\n",
    "\n",
    "While SGD works, it can be slow to converge. The **Adam** (Adaptive Moment Estimation) optimizer is a more advanced algorithm that often leads to faster training. It adapts the learning rate for each parameter individually by keeping track of two \"moments\" of the gradients:\n",
    "-   **First Moment (the mean):** This is like momentum, helping the optimizer to continue in a consistent direction.\n",
    "-   **Second Moment (the uncentered variance):** This helps to scale the learning rate, making larger updates for infrequent parameters and smaller updates for frequent ones.\n",
    "\n",
    "Here is the implementation from `from_scratch/optim.py`:\n",
    "\n",
    "```python\n",
    "# from_scratch/optim.py\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params: List[Tensor], lr: float, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n",
    "        self.t = 0\n",
    "        self.m = [np.zeros_like(p.data) for p in self.params]\n",
    "        self.v = [np.zeros_like(p.data) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is not None:\n",
    "                # Update biased moment estimates\n",
    "                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * p.grad\n",
    "                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (p.grad ** 2)\n",
    "                \n",
    "                # Compute bias-corrected estimates\n",
    "                m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                # Update parameters\n",
    "                p.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ecbcd4",
   "metadata": {},
   "source": [
    "## Training a Classifier on Real Data\n",
    "\n",
    "Let's put all our new tools together. We will train a model to classify breast cancer tumors as malignant (1) or benign (0) based on 30 different features from the Scikit-learn dataset.\n",
    "\n",
    "The training process is almost identical to our regression example, showcasing the power of our modular design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5febcc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Start ---\n",
      "Epoch 0, Loss: 1.0902\n",
      "Epoch 20, Loss: 0.1138\n",
      "Epoch 40, Loss: 0.0743\n",
      "Epoch 60, Loss: 0.0582\n",
      "Epoch 80, Loss: 0.0475\n",
      "Epoch 99, Loss: 0.0408\n",
      "\n",
      "Final Test Accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from from_scratch.autograd.tensor import Tensor\n",
    "from from_scratch.nn import Sequential, Linear, ReLU\n",
    "from from_scratch.functional import sigmoid, binary_cross_entropy\n",
    "from from_scratch.optim import Adam\n",
    "\n",
    "# 1. Load and Prepare Data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- A crucial step: Feature Scaling ---\n",
    "# Neural networks train best when input features are on a similar scale.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to our custom Tensors\n",
    "X_train_t = Tensor(X_train)\n",
    "y_train_t = Tensor(y_train.reshape(-1, 1))\n",
    "X_test_t = Tensor(X_test)\n",
    "\n",
    "# 2. Define the Model, Optimizer, and Loss\n",
    "input_features = X_train.shape[1]\n",
    "model = Sequential(\n",
    "    Linear(input_features, 16),\n",
    "    ReLU(),\n",
    "    Linear(16, 1)\n",
    ")\n",
    "optimizer = Adam(params=model.parameters(), lr=0.01)\n",
    "loss_function = binary_cross_entropy\n",
    "\n",
    "# 3. The Training Loop\n",
    "epochs = 100\n",
    "print(\"--- Training Start ---\")\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: get the raw model output (logits)\n",
    "    logits = model(X_train_t)\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    predictions = sigmoid(logits)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(predictions, y_train_t)\n",
    "    \n",
    "    # Backward pass to compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights using the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.data.item():.4f}\")\n",
    "\n",
    "# 4. Evaluate the trained model on the test set\n",
    "logits_test = model(X_test_t)\n",
    "preds_test = sigmoid(logits_test)\n",
    "\n",
    "# Convert probabilities to binary class predictions (0 or 1)\n",
    "binary_preds = (preds_test.data > 0.5).astype(int).flatten()\n",
    "accuracy = accuracy_score(y_test, binary_preds)\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e881b0e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With just a few additions to our library, we were able to build a multi-layer perceptron and achieve very high accuracy on a real-world classification task. We've shown that our from-scratch `Module`, `Linear`, `Sequential`, `Adam`, and `BCELoss` components all work together seamlessly.\n",
    "\n",
    "We have built a solid foundation for feed-forward networks. The next logical step is to explore architectures that can handle sequential data, like sentences or time series. In the next post, we will implement our first **Recurrent Neural Network (RNN)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bare-bones-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
