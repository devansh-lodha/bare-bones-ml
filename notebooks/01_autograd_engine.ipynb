{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5149eee5",
   "metadata": {},
   "source": [
    "# BareBonesML Part 1: Building an Autograd Engine from Scratch\n",
    "\n",
    "> “What I cannot create, I do not understand.” — Richard Feynman\n",
    "\n",
    "Welcome to the first post in the BareBonesML series! This quote embodies the spirit of this project. To master powerful libraries like PyTorch, we must understand whats happening under the hood. In this post we will implement an **automatic differentiation engine**, or \"autograd\" for short.\n",
    "\n",
    "### The \"Why\": Gradient Descent\n",
    "\n",
    "At its core, training a neural network is an optimization problem. We define a **loss function** that measures how \"wrong\" our model's predictions are. Our goal is to tweak the model's internal parameters (its weights and biases) to make this loss as small as possible. The most common way to do this is with an algorithm called **Gradient Descent**.\n",
    "\n",
    "Imagine your loss function is a giant, hilly landscape, and your goal is to find the lowest valley. Gradient descent tells you which direction is \"downhill\" from your current position. This \"downhill\" direction is given by the **gradient**—a vector of partial derivatives of the loss with respect to each of the model's parameters. The update rule is simple:\n",
    "\n",
    "$$\n",
    "\\text{parameter}_{\\text{new}} \\leftarrow \\text{parameter}_{\\text{old}} - \\text{learning\\_rate} \\cdot \\frac{\\partial \\text{Loss}}{\\partial \\text{parameter}_{\\text{old}}}\n",
    "$$\n",
    "\n",
    "The challenge is, how do we calculate $\\frac{\\partial \\text{Loss}}{\\partial \\text{parameter}}$ for potentially millions of parameters in a complex model? The answer is to have the machine do it for us, automatically. That's what our autograd engine will do.\n",
    "\n",
    "In this post, we will:\n",
    "1.  Explore the concept of a **computational graph**.\n",
    "2.  Design and implement our core `Tensor` and `Function` classes.\n",
    "3.  Walk through the code for fundamental operations like addition, multiplication, and power.\n",
    "4.  Unpack the logic of **backpropagation** and the chain rule.\n",
    "5.  Build a working engine that can compute gradients for any arbitrary combination of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b3da0",
   "metadata": {},
   "source": [
    "## Design Decision: Separating Data from Operations\n",
    "\n",
    "Our first major design decision is to separate the data containers from the mathematical operations. This is a powerful and clean design used by major frameworks.\n",
    "\n",
    "-   **`Tensor`**: A `Tensor` will be our data container. It will hold a NumPy array of the data itself, but more importantly, it will know its own history—which operation (`Function`) created it.\n",
    "-   **`Function`**: A `Function` will represent a single mathematical operation (e.g., addition). It will be stateless and only know how to compute its output (`forward`) and its local derivatives (`backward`).\n",
    "\n",
    "Let's look at the actual code from our library in `from_scratch/autograd/tensor.py`.\n",
    "\n",
    "### The `Function` Base Class\n",
    "\n",
    "This is the abstract parent for all our operations. Its most important part is the `apply` classmethod. It takes `Tensor` objects as input, creates an instance of the specific `Function` subclass (like `Add` or `Mul`), performs the forward calculation on the raw data, and returns a *new* `Tensor` whose `_creator` attribute points back to the function instance. This is how the graph gets built, one link at a time.\n",
    "\n",
    "```python\n",
    "# from_scratch/autograd/tensor.py\n",
    "\n",
    "class Function:\n",
    "    \"\"\"\n",
    "    Base class for differentiable operations. It links Tensors in the computational graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, *tensors: \"Tensor\"):\n",
    "        self.parents = tensors\n",
    "        self.saved_tensors: List[np.ndarray] = []\n",
    "\n",
    "    def save_for_backward(self, *tensors: np.ndarray):\n",
    "        \"\"\"Saves given tensors for use in the backward pass.\"\"\"\n",
    "        self.saved_tensors.extend(tensors)\n",
    "\n",
    "    def forward(self, *args: Any, **kwargs: Any) -> np.ndarray:\n",
    "        \"\"\"Performs the forward computation. Must be implemented by subclasses.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> Union[None, np.ndarray, Tuple[Optional[np.ndarray], ...]]:\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs. Must be implemented by subclasses.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def apply(cls, *args: \"Tensor\", **kwargs: Any) -> \"Tensor\":\n",
    "        \"\"\"Applies the function, creating a new tensor that tracks the operation.\"\"\"\n",
    "        ctx = cls(*args)\n",
    "        output_data = ctx.forward(*(t.data for t in args), **kwargs)\n",
    "        return Tensor(output_data, requires_grad=any(t.requires_grad for t in args), _creator=ctx)\n",
    "```\n",
    "\n",
    "### The `Tensor` Class\n",
    "\n",
    "Here is the core of our `Tensor` class. Notice how simple the `__init__` method is. It just stores the data and the (optional) creator function.\n",
    "\n",
    "```python\n",
    "# from_scratch/autograd/tensor.py\n",
    "\n",
    "class Tensor:\n",
    "    \"\"\"The core data structure for the autograd engine.\"\"\"\n",
    "    def __init__(self, data: Union[np.ndarray, float, int, list], requires_grad: bool = False, _creator: Optional[Function] = None):\n",
    "        if not isinstance(data, np.ndarray):\n",
    "            data = np.array(data, dtype=np.float32)\n",
    "        self.data = data\n",
    "        self.requires_grad = requires_grad\n",
    "        self._creator = _creator\n",
    "        self.grad: Optional[np.ndarray] = None\n",
    "    \n",
    "    # ... more methods to come ...\n",
    "```\n",
    "\n",
    "### A Quick Detour: What is `raise NotImplementedError`?\n",
    "\n",
    "You'll notice that the `forward` and `backward` methods in our base `Function` class don't actually do anything except `raise NotImplementedError`. This looks strange, but it's a very important and deliberate design choice.\n",
    "\n",
    "This technique turns our `Function` class into an **abstract base class**—a blueprint or a contract. By doing this, we are saying:\n",
    "\n",
    "> \"Any new, concrete operation that inherits from `Function` **must** provide its own implementation of `forward` and `backward`. If a developer forgets to do this, their code will crash immediately with an error, telling them exactly what they need to fix.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3735e8",
   "metadata": {},
   "source": [
    "## Implementing Core Operations\n",
    "\n",
    "Let's implement a few fundamental operations to see the pattern. Each one is a subclass of `Function`.\n",
    "\n",
    "### Addition (`Add`)\n",
    "\n",
    "-   **Forward Pass**: The forward pass is simple element-wise addition: $f(x, y) = x + y$.\n",
    "-   **Backward Pass**: The partial derivatives are both 1 ($\\frac{\\partial f}{\\partial x} = 1$, $\\frac{\\partial f}{\\partial y} = 1$). By the chain rule, this means the `Add` operation just passes the incoming gradient through to both of its parents.\n",
    "\n",
    "```python\n",
    "# from_scratch/autograd/tensor.py\n",
    "\n",
    "class Add(Function):\n",
    "    def forward(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        return x + y\n",
    "    def backward(self, grad: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return grad, grad\n",
    "```\n",
    "\n",
    "### Multiplication (`Mul`)\n",
    "\n",
    "-   **Forward Pass**: Element-wise multiplication: $f(x, y) = x \\cdot y$.\n",
    "-   **Backward Pass**: Using the product rule, the partial derivatives are $\\frac{\\partial f}{\\partial x} = y$ and $\\frac{\\partial f}{\\partial y} = x$. To do this, we must first save the original inputs `x` and `y` during the forward pass.\n",
    "\n",
    "```python\n",
    "# from_scratch/autograd/tensor.py\n",
    "\n",
    "class Mul(Function):\n",
    "    def forward(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        self.save_for_backward(x, y)\n",
    "        return x * y\n",
    "    def backward(self, grad: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        x, y = self.saved_tensors\n",
    "        return grad * y, grad * x\n",
    "```\n",
    "\n",
    "### Power (`Pow`)\n",
    "\n",
    "-   **Forward Pass**: Element-wise power: $f(x) = x^{\\text{power}}$.\n",
    "-   **Backward Pass**: Using the power rule, the derivative is $\\frac{\\partial f}{\\partial x} = \\text{power} \\cdot x^{\\text{power}-1}$.\n",
    "\n",
    "```python\n",
    "# from_scratch/autograd/tensor.py\n",
    "\n",
    "class Pow(Function):\n",
    "    def forward(self, x: np.ndarray, *, power: float) -> np.ndarray:\n",
    "        self.save_for_backward(x)\n",
    "        self.power = power\n",
    "        return x ** power\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        x, = self.saved_tensors\n",
    "        return grad * (self.power * (x ** (self.power - 1)))\n",
    "```\n",
    "\n",
    "To make these easy to use, we overload the operators `+`, `*`, and `**` on the `Tensor` class.\n",
    "\n",
    "```python\n",
    "# In the Tensor class in from_scratch/autograd/tensor.py\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, Tensor): other = Tensor(other)\n",
    "        return Add.apply(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(other, Tensor): other = Tensor(other)\n",
    "        return Mul.apply(self, other)\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        if not isinstance(power,(int,float)): raise TypeError(\"Power must be scalar\")\n",
    "        return Pow.apply(self, power=power)\n",
    "```\n",
    "Let's test this by building a small computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e78fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor c: Tensor(data=12.0, requires_grad=True)\n",
      "c was created by: Mul\n",
      "Tensor d: Tensor(data=16.0, requires_grad=True)\n",
      "d was created by: Add\n"
     ]
    }
   ],
   "source": [
    "# --- A small demonstration ---\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from from_scratch.autograd.tensor import Tensor\n",
    "\n",
    "a = Tensor(3.0, requires_grad=True)\n",
    "b = Tensor(4.0, requires_grad=True)\n",
    "\n",
    "# c = a * b\n",
    "# d = c + b\n",
    "c = a * b\n",
    "d = c + b\n",
    "\n",
    "print(f\"Tensor c: {c}\")\n",
    "print(f\"c was created by: {c._creator.__class__.__name__}\")\n",
    "print(f\"Tensor d: {d}\")\n",
    "print(f\"d was created by: {d._creator.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c9c9f",
   "metadata": {},
   "source": [
    "## Backpropagation: The Algorithm\n",
    "\n",
    "Now for the main event. The `backward()` method automates the chain rule. It works by first creating a topologically sorted list of all the nodes that lead to the final output. This ensures that when we process a node, we have already computed the gradients for all the nodes that depend on it.\n",
    "\n",
    "Here is the full implementation of the `backward` method in our `Tensor` class. Read the comments carefully to understand the flow.\n",
    "\n",
    "```python\n",
    "# from_scratch/autograd/tensor.py\n",
    "\n",
    "def backward(self, grad: Optional[np.ndarray] = None):\n",
    "    # 1. Ensure the tensor requires a gradient.\n",
    "    if not self.requires_grad:\n",
    "        return\n",
    "    \n",
    "    # 2. Bootstrap the gradient. For the final loss, this is 1.0.\n",
    "    if grad is None:\n",
    "        if self.data.size != 1:\n",
    "            raise ValueError(\"Gradient must be specified for non-scalar Tensors.\")\n",
    "        grad = np.ones_like(self.data, dtype=np.float32)\n",
    "    \n",
    "    self.grad = grad\n",
    "    \n",
    "    # 3. Build a topologically sorted list of all nodes in the graph.\n",
    "    visited, topo_nodes = set(), []\n",
    "    def build_topo(node):\n",
    "        if id(node) not in visited:\n",
    "            visited.add(id(node))\n",
    "            if node._creator:\n",
    "                for p in node._creator.parents:\n",
    "                    build_topo(p)\n",
    "                topo_nodes.append(node)\n",
    "    build_topo(self)\n",
    "\n",
    "    # 4. Propagate gradients backward through the graph.\n",
    "    for node in reversed(topo_nodes):\n",
    "        if not node._creator or node.grad is None:\n",
    "            continue\n",
    "\n",
    "        # Get the gradients for this node's parents by calling its creator's backward method.\n",
    "        parent_grads = node._creator.backward(node.grad)\n",
    "        \n",
    "        if not isinstance(parent_grads, tuple):\n",
    "            parent_grads = (parent_grads,)\n",
    "\n",
    "        # 5. Distribute and accumulate gradients to the parents.\n",
    "        for p, p_g in zip(node._creator.parents, parent_grads):\n",
    "            if p_g is not None and p.requires_grad:\n",
    "                # Handle cases where broadcasting occurred during the forward pass.\n",
    "                if p_g.shape != p.shape:\n",
    "                    # (Implementation detail for broadcasting omitted for clarity)\n",
    "                    pass \n",
    "                \n",
    "                # Accumulate gradient.\n",
    "                if p.grad is None:\n",
    "                    p.grad = p_g.copy()\n",
    "                else:\n",
    "                    p.grad += p_g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b74710f",
   "metadata": {},
   "source": [
    "## Final Demonstration\n",
    "\n",
    "Let's test our complete engine on a more complex expression: $L = (a \\cdot b) + a^2$.\n",
    "We need to calculate $\\frac{\\partial L}{\\partial a}$ and $\\frac{\\partial L}{\\partial b}$.\n",
    "\n",
    "Using calculus:\n",
    "-   $\\frac{\\partial L}{\\partial a} = \\frac{\\partial (ab)}{\\partial a} + \\frac{\\partial (a^2)}{\\partial a} = b + 2a$\n",
    "-   $\\frac{\\partial L}{\\partial b} = \\frac{\\partial (ab)}{\\partial b} + \\frac{\\partial (a^2)}{\\partial b} = a + 0 = a$\n",
    "\n",
    "If we set $a=3$ and $b=4$:\n",
    "-   $\\frac{\\partial L}{\\partial a} = 4 + 2 \\cdot 3 = 10$\n",
    "-   $\\frac{\\partial L}{\\partial b} = 3$\n",
    "\n",
    "Let's see if our engine gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411e912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final value L is: 21.00\n",
      "--- Gradients ---\n",
      "dL/da: 10.00 (Expected: 10.0)\n",
      "dL/db: 3.00 (Expected: 3.0)\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(3.0, requires_grad=True)\n",
    "b = Tensor(4.0, requires_grad=True)\n",
    "\n",
    "# L = (a * b) + a**2\n",
    "term1 = a * b\n",
    "term2 = a ** 2.0\n",
    "L = term1 + term2\n",
    "\n",
    "# Kick off backpropagation from the final node\n",
    "L.backward()\n",
    "\n",
    "print(f\"The final value L is: {L.data.item():.2f}\")\n",
    "print(\"--- Gradients ---\")\n",
    "print(f\"dL/da: {a.grad.item():.2f} (Expected: 10.0)\")\n",
    "print(f\"dL/db: {b.grad.item():.2f} (Expected: 3.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ba7b5",
   "metadata": {},
   "source": [
    "It works perfectly! Our engine correctly calculated the gradients, and critically, it correctly accumulated the gradients for `a` from both the `Mul` and `Pow` branches of the computational graph.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have successfully built the core of a deep learning library. By separating `Tensor` objects (which hold state) from `Function` objects (which define operations), we created a system that can dynamically build a computational graph and automatically compute the gradients of any output with respect to its inputs.\n",
    "\n",
    "This autograd engine is the fundamental tool that enables model training. In the next post, we will use this engine to build our first neural network layers and train a model to solve a real problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bare-bones-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
