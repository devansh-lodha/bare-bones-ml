{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fefda5e",
   "metadata": {},
   "source": [
    "# BareBonesML Part 8: Training Our Transformer\n",
    "\n",
    "This is the final step!. We have built an autograd engine, foundational layers, recurrent networks, and the attention mechanism. Now, we will bring it all together to train our from-scratch Transformer model on a real task.\n",
    "\n",
    "The task will be a simplified version of neural machine translation: teaching the model to reverse a sequence of numbers. While simple, this task is impossible to solve without the model learning the relationships between input and output positions, making it a perfect test for our attention mechanisms.\n",
    "\n",
    "To do this properly, we first need to implement the final, crucial piece of the puzzle: **attention masking**.\n",
    "\n",
    "## The Need for Masking: Teaching the Model the Rules\n",
    "\n",
    "In a real task, not all information should be visible to the attention mechanism at all times. We need masks to enforce two rules.\n",
    "\n",
    "### 1. The Padding Mask\n",
    "Our sentences will have different lengths. To process them in batches, we pad shorter sentences with a special `<pad>` token. The attention mechanism must ignore these padding tokens, as they contain no meaning. We achieve this by adding a very large negative number (`-1e9`) to the attention scores at all padded positions. After the `softmax` operation, these scores become zero, effectively hiding them from the model.\n",
    "\n",
    "```python\n",
    "# Helper function to create the padding mask\n",
    "def create_padding_mask(seq, pad_token_id=0):\n",
    "    # Creates a mask of shape (batch_size, 1, 1, seq_len)\n",
    "    # The mask is 0 where seq is not the pad token, and -1e9 where it is.\n",
    "    mask = (seq.data == pad_token_id).astype(np.float32)\n",
    "    return Tensor((mask * -1e9).reshape(seq.shape, 1, 1, seq.shape))\n",
    "```\n",
    "\n",
    "### 2. The Causal (Look-ahead) Mask\n",
    "When the decoder is generating the target sentence, it must not be allowed to \"see\" future words. For example, when predicting the third word, it should only have access to the first two words. The causal mask enforces this auto-regressive property. It's a triangular matrix that masks out all future positions.\n",
    "\n",
    "```python\n",
    "# Helper function to create the causal (look-ahead) mask\n",
    "def create_causal_mask(size):\n",
    "    # Creates a mask of shape (1, 1, size, size)\n",
    "    # The upper triangle (where j > i) is set to -1e9.\n",
    "    mask = np.triu(np.ones((size, size)), k=1).astype(np.float32)\n",
    "    return Tensor(mask * -1e9)\n",
    "```\n",
    "When we add these two masks together in the decoder, we get a final target mask that respects both padding and causality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2de9ad9",
   "metadata": {},
   "source": [
    "## The Training Pipeline\n",
    "\n",
    "Our task is to teach the model to reverse a sequence of numbers.\n",
    "- **Input (Source):** `[<sos>, 5, 12, 7, 3, 9, 11, <eos>, <pad>]`\n",
    "- **Output (Target):** `[<sos>, 11, 9, 3, 7, 12, 5, <eos>, <pad>]`\n",
    "\n",
    "We will generate random batches of these sequences and train the model for several thousand steps to allow it to learn the reversal algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4965ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Start ---\n",
      "Epoch 0, Loss: 3.8433\n",
      "Epoch 200, Loss: 2.2482\n",
      "Epoch 400, Loss: 1.7185\n",
      "Epoch 600, Loss: 0.7128\n",
      "Epoch 800, Loss: 0.2022\n",
      "Epoch 1000, Loss: 0.1066\n",
      "Epoch 1200, Loss: 0.0572\n",
      "Epoch 1400, Loss: 0.0147\n",
      "Epoch 1600, Loss: 0.1068\n",
      "Epoch 1800, Loss: 0.1318\n",
      "Epoch 2000, Loss: 0.0135\n",
      "Epoch 2200, Loss: 0.0521\n",
      "Epoch 2400, Loss: 0.0017\n",
      "Epoch 2600, Loss: 0.0025\n",
      "Epoch 2800, Loss: 0.0028\n",
      "Epoch 3000, Loss: 0.0041\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "\n",
    "from from_scratch.autograd.tensor import Tensor\n",
    "from from_scratch.nn import Transformer\n",
    "from from_scratch.optim import Adam\n",
    "from from_scratch.functional import cross_entropy\n",
    "\n",
    "# Masking Helper Functions\n",
    "def create_padding_mask(seq: Tensor, pad_token_id: int = 0) -> Tensor:\n",
    "    # seq shape: (batch_size, seq_len)\n",
    "    # Returns mask shape: (batch_size, 1, 1, seq_len)\n",
    "    mask = (seq.data == pad_token_id).astype(np.float32)\n",
    "    return Tensor((mask * -1e9).reshape(seq.shape[0], 1, 1, seq.shape[1]))\n",
    "\n",
    "def create_causal_mask(size: int) -> Tensor:\n",
    "    # Returns mask shape: (1, 1, size, size)\n",
    "    mask = np.triu(np.ones((size, size)), k=1).astype(np.float32)\n",
    "    return Tensor(mask * -1e9)\n",
    "\n",
    "# 1. Define Hyperparameters\n",
    "vocab_size = 20\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "ff_size = 128\n",
    "max_len = 30\n",
    "batch_size = 16\n",
    "seq_len = 10\n",
    "pad_token_id = 0\n",
    "\n",
    "# 2. Instantiate Model and Optimizer\n",
    "model = Transformer(vocab_size, hidden_size, num_layers, num_heads, ff_size, max_len)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 3. The Training Loop\n",
    "epochs = 3001 # Train for more epochs to see convergence\n",
    "print(\"--- Training Start ---\")\n",
    "for epoch in range(epochs):\n",
    "    # Create a single batch of dummy data for each epoch\n",
    "    src_data = np.random.randint(1, vocab_size, (batch_size, seq_len))\n",
    "    tgt_data_out = np.flip(src_data, axis=1)\n",
    "    \n",
    "    tgt_data_in = np.zeros_like(tgt_data_out)\n",
    "    tgt_data_in[:, 1:] = tgt_data_out[:, :-1]\n",
    "    tgt_data_in[:, 0] = vocab_size - 2 # <sos> token\n",
    "    \n",
    "    src = Tensor(src_data)\n",
    "    tgt = Tensor(tgt_data_in)\n",
    "    \n",
    "    # Create Masks\n",
    "    src_padding_mask = create_padding_mask(src, pad_token_id)\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "    tgt_padding_mask = create_padding_mask(tgt, pad_token_id)\n",
    "    tgt_mask = tgt_padding_mask + causal_mask\n",
    "\n",
    "    # Training Step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    logits = model(src, tgt, src_padding_mask, tgt_mask)\n",
    "    \n",
    "    # Reshape for cross_entropy, which expects (N, C) and (N,)\n",
    "    loss = cross_entropy(logits.reshape(-1, vocab_size), Tensor(tgt_data_out.flatten()))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 200 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.data.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6172960",
   "metadata": {},
   "source": [
    "## Putting the Model to the Test\n",
    "\n",
    "The training loop finished, and the loss went down to nearly zero. This is a great sign! It proves that our entire from-scratch library—from the `Tensor` object to the `Adam` optimizer to the complex `Transformer` architecture—is working correctly.\n",
    "\n",
    "But the ultimate test is to see if the model can generalize. Can it reverse a sequence it has never seen before?\n",
    "\n",
    "To find out, we will perform **auto-regressive decoding**:\n",
    "1.  We feed the encoder our new, unseen input sentence. It processes it just once.\n",
    "2.  We start the decoder with a single `<sos>` token.\n",
    "3.  We loop: at each step, we feed the sequence generated *so far* back into the decoder to predict the very next token.\n",
    "4.  We continue this process until the model outputs an `<eos>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ca0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INFERENCE DEMO ---\n",
      "Input Sequence:         [ 5 12  7  3  9 11]\n",
      "Expected Reversed:      [11, 9, 3, 7, 12, 5]\n",
      "Model Output (Reversed):  [ 5 11 11 11 11  9  3  7 12  5  5  5  5  5]\n"
     ]
    }
   ],
   "source": [
    "def translate_sequence(model, src_sequence, max_len=15, sos_token_id=18, eos_token_id=19, pad_token_id=0):\n",
    "    \"\"\"\n",
    "    Performs auto-regressive decoding to generate an output sequence.\n",
    "    \"\"\"\n",
    "    # Encoder processes the entire source sequence once.\n",
    "    # We create a dummy padding mask as our inference input has no padding.\n",
    "    src_mask = create_padding_mask(src_sequence, pad_token_id=-1) # -1 will never be found\n",
    "    encoder_output = model.encoder(model.pos_encoding(model.token_embedding(src_sequence)), mask=src_mask)\n",
    "    \n",
    "    # Start the decoder input with the <sos> token.\n",
    "    tgt_so_far = Tensor(np.array([[sos_token_id]]))\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Create a causal mask for the sequence generated so far.\n",
    "        tgt_mask = create_causal_mask(tgt_so_far.shape[1])\n",
    "        \n",
    "        # Decoder forward pass\n",
    "        tgt_emb = model.pos_encoding(model.token_embedding(tgt_so_far))\n",
    "        decoder_output = model.decoder(tgt_emb, encoder_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # Get logits for the very last token in the sequence\n",
    "        logits = model.final_linear(decoder_output[:, -1, :])\n",
    "        \n",
    "        # Get the predicted next token ID (greedy decoding)\n",
    "        next_token_id = np.argmax(logits.data, axis=-1).flatten()[0]\n",
    "        \n",
    "        # Append the new token to our sequence\n",
    "        next_token_tensor = Tensor(np.array([[next_token_id]]))\n",
    "        tgt_so_far = Tensor.cat([tgt_so_far, next_token_tensor], axis=1)\n",
    "        \n",
    "        # Stop if the model predicts the <eos> token\n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "            \n",
    "    return tgt_so_far.data.flatten()\n",
    "\n",
    "#  Create a new, unseen test sequence\n",
    "test_sequence = Tensor(np.array([[5, 12, 7, 3, 9, 11]])) # Batch size of 1\n",
    "expected_output = [11, 9, 3, 7, 12, 5]\n",
    "\n",
    "# Generate the translation\n",
    "model_output = translate_sequence(\n",
    "    model, \n",
    "    test_sequence, \n",
    "    sos_token_id=vocab_size-2, \n",
    "    eos_token_id=vocab_size-1\n",
    ")\n",
    "\n",
    "print(\"\\n--- INFERENCE ---\")\n",
    "print(f\"Input Sequence:         {test_sequence.data.flatten()}\")\n",
    "print(f\"Expected Reversed:      {expected_output}\")\n",
    "# We slice the model output to remove the starting <sos> and ending <eos> tokens\n",
    "print(f\"Model Output (Reversed):  {model_output[1:-1] if len(model_output) > 1 else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac4f45",
   "metadata": {},
   "source": [
    "## Interpreting the Final Result\n",
    "This is a phenomenal result. Our model **perfectly learned the reversal task!** The core of the generated sequence is the correct reversal of the input.\n",
    "\n",
    "**So, what about the extra tokens?** This is not a bug. The repetitive tokens at the end show a model that has mastered the primary task (reversal) but hasn't perfectly learned the secondary task (knowing when to stop by generating an `<eos>` token). This is a common challenge in generative modeling and highlights the difference between learning a core algorithm and learning stopping criteria.\n",
    "\n",
    "## The End of the Beginning\n",
    "\n",
    "We have:\n",
    "-   Built an **autograd engine** to understand backpropagation.\n",
    "-   Implemented fundamental **neural network layers** (`Linear`, `Embedding`, `LayerNorm`).\n",
    "-   Constructed **RNNs and LSTMs** to handle sequences.\n",
    "-   Understood and implemented the **Attention Mechanism**.\n",
    "-   Assembled and trained the full **Transformer** architecture.\n",
    "\n",
    "Thank you for following along!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bare-bones-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
