{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377c522b",
   "metadata": {},
   "source": [
    "# BareBonesML Part 7: Assembling the Full Transformer\n",
    "\n",
    "We have arrived at the summit. After building our autograd engine, foundational layers, recurrent networks, and the revolutionary attention mechanism, we now have all the necessary components to construct a full **Transformer** model, as detailed in the seminal paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "This is the moment where all our previous work—`Tensor`, `Function`, `Module`, `Linear`, `MultiHeadAttention`—comes together.\n",
    "\n",
    "![Transformer Architecture](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)\n",
    "\n",
    "Our goals for this post are:\n",
    "1.  Implement the final missing piece: **Positional Encoding**.\n",
    "2.  Explain the crucial sub-layer components: `LayerNorm`, `FeedForward`, and the residual \"Add & Norm\" connection.\n",
    "3.  Combine these components into `EncoderLayer` and `DecoderLayer` modules.\n",
    "4.  Assemble the final `Transformer` model and verify its structural integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800b6b5",
   "metadata": {},
   "source": [
    "## The Final Missing Piece: Positional Encoding\n",
    "\n",
    "The self-attention mechanism is \"permutation-invariant\"—it treats an input sentence as a \"bag of words\" with no inherent order. To fix this, we must explicitly inject information about the position of each token into its embedding.\n",
    "\n",
    "The authors of the Transformer paper proposed a clever trick using sine and cosine functions of different frequencies:\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "This creates a unique positional signature for each token, and the wave-like nature of the functions allows the model to easily learn relative positions. We pre-calculate these values and simply add them to our token embeddings.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "class PositionalEncoding(Module):\n",
    "    \"\"\"Injects positional information into the input embeddings.\"\"\"\n",
    "    def __init__(self, hidden_size: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        pe = np.zeros((max_len, hidden_size), dtype=np.float32)\n",
    "        position = np.arange(0, max_len, dtype=np.float32).reshape(-1, 1)\n",
    "        div_term = np.exp(np.arange(0, hidden_size, 2, dtype=np.float32) * -(np.log(10000.0) / hidden_size))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        self.pe = Tensor(pe, requires_grad=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self.pe[:x.shape, :]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec3c12",
   "metadata": {},
   "source": [
    "## The Sub-layer Components\n",
    "\n",
    "Before we assemble the full `EncoderLayer` and `DecoderLayer`, let's look at the smaller utility modules that make them work.\n",
    "\n",
    "### Layer Normalization (`LayerNorm`)\n",
    "`LayerNorm` normalizes the features for each token *independently* across the hidden dimension. This helps stabilize the training of deep networks by keeping the activations in a consistent range.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "class LayerNorm(Module):\n",
    "    def __init__(self, normalized_shape: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = Tensor(np.ones(normalized_shape), requires_grad=True)\n",
    "        self.beta = Tensor(np.zeros(normalized_shape), requires_grad=True)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        mean = x.sum(axis=-1, keepdims=True) / Tensor(x.shape[-1])\n",
    "        var = ((x - mean)**2).sum(axis=-1, keepdims=True) / Tensor(x.shape[-1])\n",
    "        x_norm = (x - mean) / (var + self.eps).sqrt()\n",
    "        return self.gamma * x_norm + self.beta\n",
    "```\n",
    "\n",
    "### Feed-Forward Network (`FeedForward`)\n",
    "Each encoder and decoder layer contains a simple, fully connected feed-forward network. This is applied independently to each position. It consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "class FeedForward(Module):\n",
    "    def __init__(self, hidden_size: int, ff_size: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = Linear(hidden_size, ff_size)\n",
    "        self.linear2 = Linear(ff_size, hidden_size)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.linear2(relu(self.linear1(x)))\n",
    "```\n",
    "\n",
    "### Residual Connections (`ResidualAddAndNorm`)\n",
    "This is perhaps the most critical component. Training very deep networks is difficult because gradients can vanish as they propagate backward. **Residual connections** (or \"skip connections\") solve this by adding the input of a layer to its output (`x + sublayer(x)`). This creates a direct path for the gradient to flow, making it possible to train networks with dozens or even hundreds of layers.\n",
    "\n",
    "Our module combines this addition with a `LayerNorm` step, which is the standard pattern in the Transformer.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "class ResidualAddAndNorm(Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(hidden_size)\n",
    "    def forward(self, x: Tensor, sublayer_output: Tensor) -> Tensor:\n",
    "        return self.norm(x + sublayer_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a37a54",
   "metadata": {},
   "source": [
    "## Assembling the Encoder and Decoder Layers\n",
    "\n",
    " An **`EncoderLayer`** contains two main sub-layers: a `MultiHeadAttention` module and a `FeedForward` network, each wrapped in our `ResidualAddAndNorm` module.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "class EncoderLayer(Module):\n",
    "    def __init__(self, hidden_size, num_heads, ff_size):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.add_norm1 = ResidualAddAndNorm(hidden_size)\n",
    "        self.ff = FeedForward(hidden_size, ff_size)\n",
    "        self.add_norm2 = ResidualAddAndNorm(hidden_size)\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(q=x, k=x, v=x, mask=mask)\n",
    "        x = self.add_norm1(x, attn_output)\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.add_norm2(x, ff_output)\n",
    "        return x\n",
    "```\n",
    "\n",
    "A **`DecoderLayer`** is similar but has **three** sub-layers, including the crucial cross-attention module that looks at the encoder's output.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "class DecoderLayer(Module):\n",
    "    def __init__(self, hidden_size, num_heads, ff_size):\n",
    "        super().__init__()\n",
    "        self.masked_self_attn = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.add_norm1 = ResidualAddAndNorm(hidden_size)\n",
    "        self.enc_dec_attn = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.add_norm2 = ResidualAddAndNorm(hidden_size)\n",
    "        self.ff = FeedForward(hidden_size, ff_size)\n",
    "        self.add_norm3 = ResidualAddAndNorm(hidden_size)\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        attn_output = self.masked_self_attn(q=x, k=x, v=x, mask=tgt_mask)\n",
    "        x = self.add_norm1(x, attn_output)\n",
    "        enc_dec_attn_output = self.enc_dec_attn(q=x, k=encoder_output, v=encoder_output, mask=src_mask)\n",
    "        x = self.add_norm2(x, enc_dec_attn_output)\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.add_norm3(x, ff_output)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d9b3a",
   "metadata": {},
   "source": [
    "## The Final Assembly\n",
    "We are now ready to build the full `Transformer` class. It's simply a container for the embedding layer, the positional encoding, a stack of `Encoder` layers, a stack of `Decoder` layers, and a final `Linear` layer to produce the output logits.\n",
    "\n",
    "The most important test we can run right now is a **structural test**. We're not training the model yet; we are simply verifying that a tensor can flow through the entire complex architecture without any shape mismatches or errors. This will prove that our implementation is correctly assembled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be783ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer model instantiated successfully!\n",
      "\n",
      "Input source shape: (8, 20)\n",
      "Input target shape: (8, 20)\n",
      "\n",
      "Output logits shape: (8, 20, 1000)\n",
      "Expected output shape: (8, 20, 1000)\n",
      "\n",
      "Success! A tensor flowed through the entire Transformer architecture and produced an output of the correct shape.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "\n",
    "from from_scratch.autograd.tensor import Tensor\n",
    "from from_scratch.nn import Transformer\n",
    "\n",
    "# 1. Define Model Hyperparameters\n",
    "vocab_size = 1000   # Size of our vocabulary\n",
    "hidden_size = 64    # Dimension of embeddings and model\n",
    "num_layers = 2      # Number of Encoder/Decoder layers to stack\n",
    "num_heads = 4       # Number of attention heads\n",
    "ff_size = 128       # Hidden size of the FeedForward networks\n",
    "max_len = 50        # Max sequence length for positional encoding\n",
    "batch_size = 8\n",
    "seq_len = 20        # Length of our dummy sentences\n",
    "\n",
    "# 2. Instantiate the Full Transformer Model\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ff_size=ff_size,\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "print(\"Transformer model instantiated successfully!\")\n",
    "\n",
    "# 3. Create Dummy Data\n",
    "# Source sentence\n",
    "src_tokens = Tensor(np.random.randint(0, vocab_size, (batch_size, seq_len)))\n",
    "# Target sentence\n",
    "tgt_tokens = Tensor(np.random.randint(0, vocab_size, (batch_size, seq_len)))\n",
    "\n",
    "print(f\"\\nInput source shape: {src_tokens.shape}\")\n",
    "print(f\"Input target shape: {tgt_tokens.shape}\")\n",
    "\n",
    "# 4. Perform a Single Forward Pass ---\n",
    "# In a real scenario, we would also pass attention masks here.\n",
    "logits = model(src_tokens, tgt_tokens)\n",
    "\n",
    "print(f\"\\nOutput logits shape: {logits.shape}\")\n",
    "print(f\"Expected output shape: ({batch_size}, {seq_len}, {vocab_size})\")\n",
    "\n",
    "# 5. Verification\n",
    "assert logits.shape == (batch_size, seq_len, vocab_size)\n",
    "print(\"\\nSuccess! A tensor flowed through the entire Transformer architecture and produced an output of the correct shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2052c6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We just assembled one of the most influential deep learning architectures from the ground up, using only the components we've built in our `bare-bones-ml` library.\n",
    "\n",
    "We have proven that the complex interplay of embeddings, positional encodings, multi-head attention, residual connections, and feed-forward layers is structurally sound in our implementation.\n",
    "\n",
    "The final step in this journey is to put our model to the test: in the next and final post of this from-scratch series, we will train our Transformer on a real task, implementing the necessary masking and a full training pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bare-bones-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
