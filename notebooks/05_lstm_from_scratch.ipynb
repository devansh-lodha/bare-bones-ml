{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b981cd",
   "metadata": {},
   "source": [
    "# BareBonesML Part 5: LSTMs - Giving Your Network a Long-Term Memory\n",
    "\n",
    "In our last post, we built a Recurrent Neural Network (RNN) that successfully learned to generate names. However, we ended with a cliffhanger: the **vanishing gradient problem**. As an RNN processes long sequences, the gradient signal flowing backward in time can shrink exponentially, making it impossible for the model to learn dependencies between distant elements. A simple RNN might learn that \"q\" is often followed by \"u,\" but it would struggle to ensure a name that starts with \"Alex\" ends with \"ander\" instead of \"andra.\"\n",
    "\n",
    "This is where the **Long Short-Term Memory (LSTM)** network comes in. Invented by Hochreiter & Schmidhuber in 1997, it was a groundbreaking architecture designed specifically to combat the vanishing gradient problem and remember information over long periods.\n",
    "\n",
    "## The Core Idea: A Separate Memory Lane\n",
    "\n",
    "The genius of the LSTM is the introduction of the **cell state**, $C_t$. You can think of this as a separate, protected memory \"conveyor belt\" that runs parallel to the main hidden state. The LSTM has the ability to precisely add information to or remove information from this cell state, regulated by a series of structures called **gates**.\n",
    "\n",
    "![LSTM Diagram](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "*Credit: [Colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)*\n",
    "\n",
    "An LSTM has three main gates that control this memory conveyor belt:\n",
    "1.  **Forget Gate ($f_t$):** This gate looks at the previous hidden state ($h_{t-1}$) and the current input ($x_t$) and decides what information from the *previous cell state* ($C_{t-1}$) is no longer relevant and should be discarded. It outputs a number between 0 (completely forget) and 1 (completely keep) for each number in the cell state vector.\n",
    "    $$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "2.  **Input Gate ($i_t$):** This gate decides which *new* information from the current input we're going to store in the cell state. It has two parts: a sigmoid layer that decides which values to update, and a `tanh` layer that creates a vector of new candidate values, $\\tilde{C}_t$.\n",
    "    $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "    $$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "3.  **Updating the Cell State:** Now, we update the old cell state $C_{t-1}$ into the new cell state $C_t$. We forget the old stuff by multiplying by $f_t$, and then we add the new candidate values, scaled by how much we want to update them ($i_t$).\n",
    "    $$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "\n",
    "4.  **Output Gate ($o_t$):** Finally, we decide what our next hidden state ($h_t$) will be. This is a filtered version of our cell state. We run a sigmoid gate to decide which parts of the cell state weâ€™re going to output, and then put the cell state through `tanh` and multiply it by the output of the sigmoid gate.\n",
    "    $$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "    $$h_t = o_t * \\tanh(C_t)$$\n",
    "\n",
    "This gated mechanism allows gradients to flow much more freely through time, solving the vanishing gradient problem.\n",
    "\n",
    "## Implementing the `LSTMBlock`\n",
    "\n",
    "Let's translate this directly into code in our `from_scratch/nn.py` file. Each gate will have its own weight matrix and bias vector. The `forward` method will implement the four equations above.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "\n",
    "class LSTMBlock(Module):\n",
    "    \"\"\"A Long Short-Term Memory (LSTM) block.\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        concat_size = input_size + hidden_size\n",
    "        \n",
    "        # Forget gate weights\n",
    "        self.W_f = Tensor(np.random.randn(concat_size, hidden_size) * 0.01, requires_grad=True)\n",
    "        self.b_f = Tensor(np.zeros(hidden_size), requires_grad=True)\n",
    "        \n",
    "        # Input gate weights\n",
    "        self.W_i = Tensor(np.random.randn(concat_size, hidden_size) * 0.01, requires_grad=True)\n",
    "        self.b_i = Tensor(np.zeros(hidden_size), requires_grad=True)\n",
    "        \n",
    "        # Candidate cell state weights\n",
    "        self.W_c = Tensor(np.random.randn(concat_size, hidden_size) * 0.01, requires_grad=True)\n",
    "        self.b_c = Tensor(np.zeros(hidden_size), requires_grad=True)\n",
    "        \n",
    "        # Output gate weights\n",
    "        self.W_o = Tensor(np.random.randn(concat_size, hidden_size) * 0.01, requires_grad=True)\n",
    "        self.b_o = Tensor(np.zeros(hidden_size), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: Tensor, h_prev: Tensor, c_prev: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Performs one step of the LSTM computation.\"\"\"\n",
    "        combined = Tensor.cat([x, h_prev], axis=1)\n",
    "        \n",
    "        # f_t = forget_gate\n",
    "        f_t = sigmoid((combined @ self.W_f) + self.b_f)\n",
    "        \n",
    "        # i_t = input_gate\n",
    "        i_t = sigmoid((combined @ self.W_i) + self.b_i)\n",
    "        \n",
    "        # c_candidate = candidate cell state\n",
    "        c_candidate = tanh((combined @ self.W_c) + self.b_c)\n",
    "        \n",
    "        # c_next = new cell state\n",
    "        c_next = f_t * c_prev + i_t * c_candidate\n",
    "        \n",
    "        # o_t = output_gate\n",
    "        o_t = sigmoid((combined @ self.W_o) + self.b_o)\n",
    "\n",
    "        # h_next = new hidden state\n",
    "        h_next = o_t * tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        return [self.W_f, self.b_f, self.W_i, self.b_i, self.W_c, self.b_c, self.W_o, self.b_o]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e07ed9",
   "metadata": {},
   "source": [
    "## Training the LSTM for Name Generation\n",
    "\n",
    "Now, we'll run the exact same experiment as before, but we'll swap our `RecurrentBlock` with our new `LSTMBlock`. The training loop is nearly identical, but now we must initialize and pass *two* states at each time step: the hidden state `h` and the cell state `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5944e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32033 names. First 5: ['emma', 'olivia', 'ava', 'isabella', 'sophia']\n",
      "\n",
      "Vocabulary size: 27\n",
      "Characters: .abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from from_scratch.autograd.tensor import Tensor\n",
    "\n",
    "# 1. The Dataset\n",
    "with open('data/names.txt', 'r') as f:\n",
    "    names = [name.lower() for name in f.read().splitlines()]\n",
    "\n",
    "\n",
    "if names:\n",
    "    print(f\"Loaded {len(names)} names. First 5: {names[:5]}\")\n",
    "\n",
    "    # 2. Create the Vocabulary\n",
    "    chars = sorted(list(set(\".\" + \"\".join(names))))\n",
    "    char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "    int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "    print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "    print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "    # 3. Helper to create a training example\n",
    "    def get_random_example():\n",
    "        name = random.choice(names)\n",
    "        full_sequence = \".\" + name + \".\"\n",
    "        input_indices = [char_to_int[c] for c in full_sequence[:-1]]\n",
    "        target_indices = [char_to_int[c] for c in full_sequence[1:]]\n",
    "        input_one_hot = np.zeros((len(input_indices), vocab_size), dtype=np.float32)\n",
    "        input_one_hot[np.arange(len(input_indices)), input_indices] = 1\n",
    "        return Tensor(input_one_hot), Tensor(np.array(target_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73903b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Start ---\n",
      "Epoch 0, Avg Loss: 3.2939\n",
      "Epoch 1000, Avg Loss: 2.0206\n",
      "Epoch 2000, Avg Loss: 2.2393\n",
      "Epoch 3000, Avg Loss: 1.9994\n",
      "Epoch 4000, Avg Loss: 1.9533\n",
      "Epoch 5000, Avg Loss: 2.8938\n",
      "Epoch 6000, Avg Loss: 1.8327\n",
      "Epoch 7000, Avg Loss: 1.9813\n",
      "Epoch 8000, Avg Loss: 1.6036\n",
      "Epoch 9000, Avg Loss: 2.7477\n",
      "Epoch 10000, Avg Loss: 2.3096\n",
      "Epoch 11000, Avg Loss: 1.8515\n",
      "Epoch 12000, Avg Loss: 1.8646\n",
      "Epoch 13000, Avg Loss: 1.9118\n",
      "Epoch 14000, Avg Loss: 1.5715\n",
      "Epoch 15000, Avg Loss: 2.5192\n",
      "Epoch 16000, Avg Loss: 2.5600\n",
      "Epoch 17000, Avg Loss: 2.5066\n",
      "Epoch 18000, Avg Loss: 2.3326\n",
      "Epoch 19000, Avg Loss: 1.9452\n",
      "Epoch 19999, Avg Loss: 2.2930\n"
     ]
    }
   ],
   "source": [
    "from from_scratch.nn import LSTMBlock, Linear\n",
    "from from_scratch.optim import Adam\n",
    "from from_scratch.functional import cross_entropy\n",
    "\n",
    "# Model Definition\n",
    "hidden_size = 128\n",
    "lstm_layer = LSTMBlock(input_size=vocab_size, hidden_size=hidden_size)\n",
    "output_layer = Linear(input_size=hidden_size, output_size=vocab_size)\n",
    "\n",
    "# Group all parameters for the optimizer\n",
    "all_params = lstm_layer.parameters() + output_layer.parameters()\n",
    "optimizer = Adam(params=all_params, lr=0.005)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 20000\n",
    "print_every = 1000\n",
    "\n",
    "print(\"\\n--- Training Start ---\")\n",
    "for epoch in range(epochs):\n",
    "    input_tensor, target_tensor = get_random_example()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Initialize BOTH hidden and cell states\n",
    "    hidden = Tensor(np.zeros((1, hidden_size)))\n",
    "    cell = Tensor(np.zeros((1, hidden_size)))\n",
    "    \n",
    "    total_loss = Tensor(0.0)\n",
    "    \n",
    "    for t in range(input_tensor.shape[0]):\n",
    "        x_t = input_tensor[t:t+1, :]\n",
    "        \n",
    "        # The forward pass now returns two states\n",
    "        hidden, cell = lstm_layer(x_t, hidden, cell)\n",
    "        \n",
    "        logits = output_layer(hidden)\n",
    "        target_t = target_tensor[t:t+1]\n",
    "        loss = cross_entropy(logits, target_t)\n",
    "        total_loss = total_loss + loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    \n",
    "    for p in all_params:\n",
    "        if p.grad is not None:\n",
    "            np.clip(p.grad, -5, 5, out=p.grad)\n",
    "            \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % print_every == 0 or epoch == epochs - 1:\n",
    "        avg_loss = total_loss.data.item() / input_tensor.shape[0]\n",
    "        print(f\"Epoch {epoch}, Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ef02f",
   "metadata": {},
   "source": [
    "## Generating Names with the LSTM\n",
    "\n",
    "The inference loop is also updated to manage both the hidden and cell states, passing them from one step to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da8162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Names with LSTM ---\n",
      "adiha\n",
      "brean\n",
      "calane\n",
      "derce\n",
      "ezorn\n"
     ]
    }
   ],
   "source": [
    "from from_scratch.functional import softmax\n",
    "\n",
    "def generate_name(start_letter='a', max_len=20):\n",
    "    def char_to_tensor(char):\n",
    "        tensor = np.zeros((1, vocab_size))\n",
    "        tensor[0, char_to_int[char]] = 1\n",
    "        return Tensor(tensor)\n",
    "\n",
    "    # Initialize both hidden and cell state\n",
    "    hidden = Tensor(np.zeros((1, hidden_size)))\n",
    "    cell = Tensor(np.zeros((1, hidden_size)))\n",
    "    \n",
    "    current_input = char_to_tensor(start_letter)\n",
    "    name = start_letter\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Pass and update both states\n",
    "        hidden, cell = lstm_layer(current_input, hidden, cell)\n",
    "        logits = output_layer(hidden)\n",
    "        \n",
    "        probs = softmax(logits)\n",
    "        next_char_idx = np.random.choice(len(chars), p=probs.data.flatten())\n",
    "        \n",
    "        if int_to_char[next_char_idx] == '.':\n",
    "            break\n",
    "            \n",
    "        next_char = int_to_char[next_char_idx]\n",
    "        name += next_char\n",
    "        current_input = char_to_tensor(next_char)\n",
    "            \n",
    "    return name\n",
    "\n",
    "print(\"\\n--- Generated Names with LSTM ---\")\n",
    "for char in \"abcde\":\n",
    "    print(generate_name(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a901020",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You will likely notice that the names generated by the LSTM feel even more structured and coherent than those from the simple RNN. This is a direct result of its superior ability to capture and maintain context over time, thanks to its gated cell state.\n",
    "\n",
    "Even LSTMs, however, have limitations when dealing with extremely long sequences where context from the very distant past is important. The need to compress all past information into a fixed-size state vector remains a bottleneck.\n",
    "\n",
    "In our next post, we will explore the revolutionary concept that moved beyond this sequential bottleneck altogether: the **Attention Mechanism**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bare-bones-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
