{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f565f15",
   "metadata": {},
   "source": [
    "# BareBonesML Part 6: The Attention Mechanism - Letting Models Focus\n",
    "\n",
    "In our journey so far, we have built Recurrent Neural Networks and LSTMs. These models process sequences step-by-step, maintaining a \"memory\" or hidden state that summarizes all the information seen so far. While powerful, this approach has a fundamental weakness: the hidden state becomes an **information bottleneck**. The model must compress the entire meaning of a long sentence like \"The fluffy cat, which had been sleeping all day on the warm, sunny windowsill, finally woke up and...\" into a single, fixed-size vector. By the time the model processes \"woke up,\" the specific details about the \"fluffy cat\" might be diluted or lost.\n",
    "\n",
    "What if, instead of relying on a single summary vector, the model could \"look back\" at the entire input sequence at every step and decide which parts are most relevant for the current task?\n",
    "\n",
    "This is the core intuition behind the **Attention Mechanism**. It's a technique that allows a model to dynamically focus on the most relevant parts of the input sequence when producing a part of the output sequence. It was the key ingredient that unlocked the power of the Transformer architecture and redefined modern AI.\n",
    "\n",
    "## The Attention Formula: Queries, Keys, and Values\n",
    "\n",
    "Attention can be described beautifully through an analogy to a library retrieval system. You have a question, and you want to find the most relevant books.\n",
    "\n",
    "*   **Query (Q):** This is your question. In a model, it represents the current context or the word you are trying to produce (e.g., \"I need information about an animal\").\n",
    "*   **Key (K):** These are the titles on the spines of all the books in the library. Each input word has a Key vector, like a label that says, \"I am about animals\" or \"I am about places.\"\n",
    "*   **Value (V):** These are the actual contents of the books. Each input word also has a Value vector, which is its rich, meaningful representation.\n",
    "\n",
    "The process is intuitive:\n",
    "1. You compare your **Query** to every **Key** in the library to see how well they match. A common way to do this is with a dot product. A high score means a strong match.\n",
    "2. You take all the scores and run them through a `softmax` function. This converts the scores into a probability distribution. These are your **attention weights**. A key with a high score will get a high weight.\n",
    "3. You create a weighted sum of all the **Values** (the books' contents) using your attention weights. Books with higher weights contribute more to the final result.\n",
    "\n",
    "This is captured in the famous Scaled Dot-Product Attention formula from the \"Attention Is All You Need\" paper:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "The division by $\\sqrt{d_k}$ (the square root of the key dimension) is a scaling factor that helps stabilize the gradients during training, preventing the dot product scores from becoming too large.\n",
    "\n",
    "Let's look at our from-scratch implementation.\n",
    "\n",
    "```python\n",
    "# from_scratch/nn.py\n",
    "\n",
    "class ScaledDotProductAttention(Module):\n",
    "    \"\"\"Computes Scaled Dot-Product Attention.\"\"\"\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask=None) -> Tensor:\n",
    "        # Get the dimension of the key vectors\n",
    "        key_dim = Tensor(k.shape[-1])\n",
    "        \n",
    "        # Transpose the last two dimensions of the key tensor for matrix multiplication\n",
    "        key_transposed = k.transpose(-2, -1)\n",
    "        \n",
    "        # 1. Calculate scores: Query @ Key_transposed\n",
    "        scores = q @ key_transposed\n",
    "        \n",
    "        # 2. Scale the scores\n",
    "        scaled_scores = scores / key_dim.sqrt()\n",
    "        \n",
    "        # 3. Apply mask if provided (e.g., for padding or causal attention)\n",
    "        if mask is not None:\n",
    "            scaled_scores = scaled_scores + mask\n",
    "            \n",
    "        # 4. Apply softmax to get attention weights\n",
    "        weights = softmax(scaled_scores)\n",
    "        \n",
    "        # 5. Multiply weights by Values to get the final output\n",
    "        return weights @ v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf8ca6",
   "metadata": {},
   "source": [
    "### A Simple Demonstration\n",
    "\n",
    "Let's see this in action. We'll create a simple scenario where our \"Values\" represent three distinct concepts, and we'll watch how the attention weights shift based on our \"Query.\" To make the effect obvious, our query vector will have a large value in the dimension it's \"interested\" in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89c350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Query: 'dog' ---\n",
      "Attention Weights:\n",
      " [[0. 1. 0.]]\n",
      "The weights show a very clear focus on the second item (index 1), which is 'dog'.\n",
      "\n",
      "--- Query: 'bird' ---\n",
      "Attention Weights:\n",
      " [[0. 0. 1.]]\n",
      "The weights have decisively shifted to the third item (index 2), which is 'bird'.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from from_scratch.autograd.tensor import Tensor\n",
    "from from_scratch.functional import softmax\n",
    "\n",
    "# Imagine our \"Values\" represent three concepts: \"cat\", \"dog\", \"bird\"\n",
    "V = Tensor(np.array([\n",
    "    [1, 0, 0],  # Vector for 'cat'\n",
    "    [0, 1, 0],  # Vector for 'dog'\n",
    "    [0, 0, 1]   # Vector for 'bird'\n",
    "]))\n",
    "\n",
    "# The \"Keys\" are labels for our values. We'll make them match the values for simplicity.\n",
    "K = Tensor(np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "]))\n",
    "\n",
    "def calculate_attention(Q, K, V):\n",
    "    # For this simple demo, we'll omit the scaling factor for a clearer result.\n",
    "    scores = Q @ K.T\n",
    "    weights = softmax(scores)\n",
    "    return weights\n",
    "\n",
    "# Scenario 1: We are looking for \"dog\"\n",
    "# Use a more \"opinionated\" query vector with a large magnitude in the 'dog' dimension.\n",
    "query_dog = Tensor([[1.0, 10.0, 1.0]]) \n",
    "\n",
    "attention_weights_dog = calculate_attention(query_dog, K, V)\n",
    "\n",
    "print(\"--- Query: 'dog' ---\")\n",
    "print(\"Attention Weights:\\n\", np.round(attention_weights_dog.data, 2))\n",
    "print(\"The weights show a very clear focus on the second item (index 1), which is 'dog'.\")\n",
    "\n",
    "# Scenario 2: Now we are looking for \"bird\"\n",
    "query_bird = Tensor([[1.0, 1.0, 10.0]])\n",
    "\n",
    "attention_weights_bird = calculate_attention(query_bird, K, V)\n",
    "\n",
    "print(\"\\n--- Query: 'bird' ---\")\n",
    "print(\"Attention Weights:\\n\", np.round(attention_weights_bird.data, 2))\n",
    "print(\"The weights have decisively shifted to the third item (index 2), which is 'bird'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64646f",
   "metadata": {},
   "source": [
    "## Multi-Head Attention: Focusing on Many Things at Once\n",
    "\n",
    "The Transformer paper took this one step further with **Multi-Head Attention**. The intuition is simple: instead of having one attention mechanism, let's have several of them (\"heads\") working in parallel. Each head can learn to focus on different aspects of the input. For example, when translating a sentence, one head might learn to track subject-verb agreement, while another tracks adjective-noun pairings.\n",
    "\n",
    "This is achieved by:\n",
    "1.  Creating separate `Linear` projection layers for the Queries, Keys, and Values for each head.\n",
    "2.  Splitting the input into multiple \"heads\" and applying Scaled Dot-Product Attention to each head in parallel.\n",
    "3.  Concatenating the results from all heads.\n",
    "4.  Passing the concatenated output through a final `Linear` layer.\n",
    "\n",
    "### A Structural Test\n",
    "\n",
    "At this stage, the output of an untrained `MultiHeadAttention` module is just a matrix of meaningless numbers. The most important thing to verify is its **structure**. A key property of these blocks is that the **output shape is identical to the input shape**. This is what allows us to stack them to create a deep network. Let's test that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7fb494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (4, 10, 32)\n",
      "Output shape: (4, 10, 32)\n",
      "\n",
      "Success! The MultiHeadAttention module processed the input and produced an output of the correct shape.\n"
     ]
    }
   ],
   "source": [
    "from from_scratch.nn import MultiHeadAttention\n",
    "\n",
    "# Define parameters\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "hidden_size = 32\n",
    "num_heads = 4 # hidden_size must be divisible by num_heads\n",
    "\n",
    "# Create some dummy input tensors\n",
    "query = Tensor(np.random.randn(batch_size, seq_len, hidden_size))\n",
    "key = Tensor(np.random.randn(batch_size, seq_len, hidden_size))\n",
    "value = Tensor(np.random.randn(batch_size, seq_len, hidden_size))\n",
    "\n",
    "# Instantiate and run our module\n",
    "multi_head_attention = MultiHeadAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
    "output = multi_head_attention(q=query, k=key, v=value)\n",
    "\n",
    "print(f\"Input shape:  ({batch_size}, {seq_len}, {hidden_size})\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "assert output.shape == (batch_size, seq_len, hidden_size)\n",
    "print(\"\\nSuccess! The MultiHeadAttention module processed the input and produced an output of the correct shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79d762",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Attention mechanism is arguably the most important concept in modern deep learning behind backpropagation itself. It broke the sequential bottleneck of RNNs and paved the way for parallelizable, highly effective models.\n",
    "\n",
    "Now that we have built this core component, we are finally ready to assemble the full **Transformer** architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bare-bones-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
